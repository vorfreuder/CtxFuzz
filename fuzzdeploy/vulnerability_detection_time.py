import datetime
import json
from pathlib import Path

import pandas as pd
from pandas import DataFrame
from styleframe import StyleFrame, Styler, utils

from .utils import (
    VULNERABILITY_SEVERITY,
    get_item_path,
    is_heap_related_vulnerability,
    work_dir_iterdir,
)


def get_plot_data(plot_data_path: str | Path) -> pd.DataFrame:
    df = pd.read_csv(plot_data_path)
    df.columns = df.columns.str.strip()
    if "# relative_time" in df.columns:
        df.rename(columns={"# relative_time": "time"}, inplace=True)
        df.rename(columns={"saved_crashes": "crashes"}, inplace=True)
    if "# unix_time" in df.columns:
        df.rename(columns={"# unix_time": "time"}, inplace=True)
        df.rename(columns={"unique_crashes": "crashes"}, inplace=True)
        df["time"] = df["time"] - df["time"].iloc[0]
    df_unique = df.drop_duplicates(subset="crashes", keep="first")
    return df_unique[["time", "crashes"]]


def get(work_dir: str | Path):
    work_dir = Path(work_dir).absolute()
    assert (
        work_dir / "casr"
    ).exists(), f"casr dir not found in {work_dir}. Please run casr first"
    res = []
    for item in work_dir_iterdir(work_dir, "casr"):
        archive_path = item.work_dir / "archive" / item.fuzzer / item.target / item.idx
        plot_data_path = get_item_path(
            archive_path,
            "plot_data",
        )
        assert plot_data_path, f"{archive_path} plot_data not found"
        plot_data = get_plot_data(plot_data_path)
        reports_unique_line_path = (
            item.work_dir
            / "casr"
            / item.fuzzer
            / item.target
            / item.idx
            / "reports_unique_line"
        )
        if not reports_unique_line_path.exists():
            res.append(
                {
                    "fuzzer": item.fuzzer,
                    "target": item.target,
                    "idx": item.idx,
                }
            )
            continue
        for report in reports_unique_line_path.glob("*"):
            if not report.is_file() and not report.name.endswith(".casrep"):
                continue
            with open(report, "r") as f:
                json_data = json.load(f)
            vul_type = json_data["CrashSeverity"]["ShortDescription"]
            crash_line = json_data["CrashLine"].split("/")[-1]
            id = int(report.name.split(",")[0].lstrip("id:")) + 1
            # print("plot_data_path\n", plot_data_path)
            # print("report\n", report)
            # print("plot_data\n", plot_data)
            # print(plot_data.loc[plot_data["crashes"] <= id, :].tail(1)["time"])
            vul_detection_time = (
                plot_data.loc[plot_data["crashes"] <= id, :].tail(1)["time"].values
            )
            if len(vul_detection_time) == 0:
                vul_detection_time = plot_data.iloc[0]["time"]
            else:
                vul_detection_time = vul_detection_time[0]
            res.append(
                {
                    "fuzzer": item.fuzzer,
                    "target": item.target,
                    "idx": item.idx,
                    "vulnerability_type": vul_type,
                    "crash_line": crash_line,
                    "vulnerability_detection_time": vul_detection_time,
                }
            )
    df = pd.DataFrame(res)
    df.sort_values(
        ["vulnerability_type", "crash_line"],
        inplace=True,
    )
    return df


def _to_excel(df: DataFrame, output_path: str | Path, ignore_idx: bool = False):
    with pd.ExcelWriter(output_path, engine="openpyxl") as writer:
        for target, group in df.groupby("target"):
            group.sort_values(
                ["vulnerability_type", "crash_line"],
                inplace=True,
            )
            data = []
            aggregation_rules = {}
            exploitable_ls = set()
            probably_exploitable_ls = set()
            remaining_ls = set()
            for row in group.itertuples():
                field = f"{row.vulnerability_type}/{row.crash_line}"
                data.append(
                    {
                        "fuzzer": row.fuzzer,
                        "target": row.target,
                        "idx": row.idx,
                        field: row.vulnerability_detection_time,
                    }
                )
                aggregation_rules[field] = "min"
                if row.vulnerability_type in VULNERABILITY_SEVERITY["EXPLOITABLE"]:
                    exploitable_ls.add(field)
                elif (
                    row.vulnerability_type
                    in VULNERABILITY_SEVERITY["PROBABLY_EXPLOITABLE"]
                ):
                    probably_exploitable_ls.add(field)
                else:
                    remaining_ls.add(field)
            exploitable_ls = sorted(exploitable_ls)
            probably_exploitable_ls = sorted(probably_exploitable_ls)
            remaining_ls = sorted(remaining_ls)
            columns = (
                ["fuzzer", "target", "idx"]
                + exploitable_ls
                + probably_exploitable_ls
                + remaining_ls
            )
            # Minimum time to find vulnerabilities is preferred
            group = (
                pd.DataFrame(data)
                .groupby(["fuzzer", "target", "idx"])
                .agg(aggregation_rules)
                .reset_index()
            )
            header_end = columns.index("idx")
            if ignore_idx:
                heap_related_vul_avg_values = []
                for row in group.iterrows():
                    cnt = 0
                    for col_name in group.columns:
                        if pd.isna(row[1][col_name]):
                            continue
                        vul_type_arr = col_name.split("/", 1)
                        if len(vul_type_arr) == 1:
                            continue
                        if is_heap_related_vulnerability(vul_type_arr[0]):
                            cnt += 1
                    heap_related_vul_avg_values.append(cnt)
                group["heap_related_vul_avg"] = heap_related_vul_avg_values
                group.rename(columns={"idx": "repeat"}, inplace=True)
                aggregation_rules["repeat"] = "count"
                aggregation_rules["heap_related_vul_avg"] = "mean"
                group = (
                    group.groupby(["fuzzer", "target"])
                    .agg(aggregation_rules)
                    .reset_index()
                )
                columns[header_end] = "repeat"
                header_end += 1
                columns.insert(header_end, "heap_related_vul_avg")
            header_end += 1
            group = group[columns]
            # Translate into hours:minutes:seconds
            for col in group.columns[header_end:]:
                group[col] = group[col].map(
                    lambda seconds: str(datetime.timedelta(seconds=int(seconds))),
                    na_action="ignore",
                )
            # Count the number of vulnerabilities found
            total_values = group.iloc[:, header_end:].count(axis=1)
            group.insert(header_end, "total", total_values)
            heap_related_vul_avg_values = []
            for row in group.iterrows():
                cnt = 0
                for col_name in group.columns:
                    if pd.isna(row[1][col_name]):
                        continue
                    vul_type_arr = col_name.split("/", 1)
                    if len(vul_type_arr) == 1:
                        continue
                    if is_heap_related_vulnerability(vul_type_arr[0]):
                        cnt += 1
                heap_related_vul_avg_values.append(cnt)
            group.insert(header_end, "heap_related_vul", heap_related_vul_avg_values)
            if ignore_idx:
                group.sort_values(
                    ["heap_related_vul", "heap_related_vul_avg", "total"],
                    ascending=[False, False, False],
                    inplace=True,
                )
            else:
                group.sort_values(
                    ["heap_related_vul", "total"],
                    ascending=[False, False],
                    inplace=True,
                )

            common_style = {
                "font": utils.fonts.calibri,
                "font_size": 17,
                "wrap_text": False,
            }
            default_style = Styler(**common_style)
            sf = StyleFrame(obj=group, styler_obj=default_style)

            header_style = Styler(**common_style, bold=True)
            sf.apply_headers_style(styler_obj=header_style)

            for col, color in zip(
                [exploitable_ls, probably_exploitable_ls],
                [utils.colors.dark_red, utils.colors.dark_blue],
            ):
                header_style = Styler(
                    **common_style,
                    font_color=color,
                )
                sf.apply_headers_style(
                    styler_obj=header_style,
                    cols_to_style=col,
                )

            sf.to_excel(
                writer,
                sheet_name=str(target),
                best_fit=group.columns.tolist(),
                columns_to_hide=["target"],
                columns_and_rows_to_freeze="A2",
            )


def to_excel(work_dir: str | Path, ignore_idx: bool = False):
    work_dir = Path(work_dir).absolute()
    df = get(work_dir)
    if ignore_idx:
        excel_path = (
            work_dir / f"{work_dir.name}_vulnerability_detection_time_ignore_idx.xlsx"
        )
    else:
        excel_path = work_dir / f"{work_dir.name}_vulnerability_detection_time.xlsx"
    _to_excel(
        df=df,
        output_path=excel_path,
        ignore_idx=ignore_idx,
    )
    print(f"save vulnerability detection time to {excel_path}")


def get_triage_rule(triage_rule_path: str | Path) -> dict[str, list]:
    triage_rule_path = Path(triage_rule_path).absolute()
    triage_rule = {}
    for target in triage_rule_path.iterdir():
        if target.is_file():
            continue
        triage_rule[target.name] = []
        for alias in target.iterdir():
            report_ls = list(alias.glob("*.casrep"))
            assert len(report_ls) > 0, f"no casrep file in {alias}"
            tmp_ls = []
            for report in report_ls:
                with open(report, "r") as f:
                    json_data = json.load(f)
                    tmp_ls.append(
                        {
                            "vulnerability_type": json_data["CrashSeverity"][
                                "ShortDescription"
                            ],
                            "crash_line": json_data["CrashLine"].split("/")[-1],
                        }
                    )
            tmp_ls = sorted(
                tmp_ls, key=lambda x: (x["vulnerability_type"], x["crash_line"])
            )
            triage_rule[target.name].append(tmp_ls)
    return triage_rule


def dirs_to_excel(
    work_dirs: list[str | Path],
    output_path: str | Path,
    triage_rule_path: str | Path | None = None,
):
    res = []
    cnt = 1
    for work_dir in work_dirs:
        df = get(work_dir)
        unique_fuzzers = df["fuzzer"].unique().tolist()
        unique_targets = df["target"].unique().tolist()
        unique_idxs = df["idx"].unique().tolist()
        for fuzzer in unique_fuzzers:
            for target in unique_targets:
                for idx in unique_idxs:
                    if not (
                        (df["fuzzer"] == fuzzer)
                        & (df["target"] == target)
                        & (df["idx"] == idx)
                    ).any():
                        continue
                    df.loc[
                        (df["fuzzer"] == fuzzer)
                        & (df["target"] == target)
                        & (df["idx"] == idx),
                        "idx",
                    ] = cnt
                    cnt += 1
        if triage_rule_path:
            triage_rule = get_triage_rule(triage_rule_path)
            targets = list(triage_rule.keys())
            assert (
                len(set(unique_targets) - set(targets)) == 0
            ), f"{set(unique_targets) - set(targets)} in {work_dir} not in {triage_rule_path}"
            for target in unique_targets:
                target_triage_rule = triage_rule[target]
                if len(target_triage_rule) == 1:
                    continue
                for item in target_triage_rule:
                    first_vul_type = item[0]["vulnerability_type"]
                    first_crash_line = item[0]["crash_line"]
                    for alias in item[1:]:
                        df.loc[
                            (df["target"] == target)
                            & (df["vulnerability_type"] == alias["vulnerability_type"])
                            & (df["crash_line"] == alias["crash_line"]),
                            ["vulnerability_type", "crash_line"],
                        ] = [first_vul_type, first_crash_line]
        res.append(df)
    df = pd.concat(res, ignore_index=True)
    _to_excel(
        df=df,
        output_path=output_path,
        ignore_idx=True,
    )
    print(f"save dirs vulnerability detection time to {output_path}")
